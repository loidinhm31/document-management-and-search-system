This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-23T14:03:43.988Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Directory Structure
================================================================
src/
  main/
    java/
      com/
        dms/
          search/
            config/
              ElasticsearchConfig.java
              EventConfig.java
              ObjectMapperConfig.java
              RabbitMQConfig.java
            consumer/
              EventConsumer.java
            dto/
              DocumentContent.java
              ErrorResponse.java
              ExtractedText.java
              UserDto.java
            elasticsearch/
              repository/
                DocumentIndexRepository.java
              DocumentIndex.java
            enums/
              DocumentType.java
              EventType.java
            exception/
              InvalidDocumentException.java
              UnsupportedDocumentTypeException.java
            model/
              DocumentInformation.java
              EventRequest.java
              SyncEventRequest.java
            repository/
              DocumentRepository.java
            service/
              ContentExtractorService.java
              DocumentService.java
              LargeFileProcessor.java
              OcrLargeFileProcessor.java
              OcrService.java
              SmartPdfExtractor.java
            SearchServiceApplication.java
    resources/
      application-local.yml
      application.yml
.gitattributes
.gitignore
pom.xml
README.md

================================================================
Files
================================================================

================
File: src/main/java/com/dms/search/config/ElasticsearchConfig.java
================
package com.dms.search.config;

import org.springframework.context.annotation.Configuration;
import org.springframework.data.elasticsearch.client.ClientConfiguration;
import org.springframework.data.elasticsearch.client.elc.ElasticsearchConfiguration;
import org.springframework.data.elasticsearch.repository.config.EnableElasticsearchRepositories;

@Configuration
@EnableElasticsearchRepositories(basePackages = "com.dms.search.elasticsearch.repository")
class ElasticsearchConfig extends ElasticsearchConfiguration {

    @Override
    public ClientConfiguration clientConfiguration() {
        return ClientConfiguration.builder()
                .connectedTo("localhost:9200").build();
    }
}

================
File: src/main/java/com/dms/search/config/EventConfig.java
================
package com.dms.search.config;

import lombok.Getter;
import org.springframework.amqp.core.Binding;
import org.springframework.amqp.core.BindingBuilder;
import org.springframework.amqp.core.Queue;
import org.springframework.amqp.core.TopicExchange;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Getter
@Configuration
public class EventConfig {

    @Value("${rabbitmq.exchanges.internal}")
    private String internalExchange;

    @Value("${rabbitmq.queues.document-sync}")
    private String notificationQueue;

    @Value("${rabbitmq.routing-keys.internal-document-sync}")
    private String internalNotificationRoutingKey;

    @Bean
    public TopicExchange internalTopicExchange() {
        return new TopicExchange(this.internalExchange);
    }

    @Bean
    public Queue notificationQueue() {
        return new Queue(this.notificationQueue);
    }

    @Bean
    public Binding internalToNotificationBinding() {
        return BindingBuilder
                .bind(notificationQueue())
                .to(internalTopicExchange())
                .with(this.internalNotificationRoutingKey);
    }


}

================
File: src/main/java/com/dms/search/config/ObjectMapperConfig.java
================
package com.dms.search.config;

import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class ObjectMapperConfig {

    @Bean
    public ObjectMapper objectMapper() {
        ObjectMapper objectMapper = new ObjectMapper();
        objectMapper.configure(DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY, true);
        objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);
        objectMapper.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false);

        return objectMapper;
    }

}

================
File: src/main/java/com/dms/search/config/RabbitMQConfig.java
================
package com.dms.search.config;

import com.fasterxml.jackson.databind.ObjectMapper;
import org.springframework.amqp.rabbit.config.SimpleRabbitListenerContainerFactory;
import org.springframework.amqp.rabbit.connection.ConnectionFactory;
import org.springframework.amqp.support.converter.Jackson2JsonMessageConverter;
import org.springframework.amqp.support.converter.MessageConverter;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class RabbitMQConfig {

    private final ConnectionFactory connectionFactory;

    private final ObjectMapper objectMapper;

    public RabbitMQConfig(ConnectionFactory connectionFactory, ObjectMapper objectMapper) {
        this.connectionFactory = connectionFactory;
        this.objectMapper = objectMapper;
    }

    @Bean
    public SimpleRabbitListenerContainerFactory simpleRabbitListenerContainerFactory() {
        SimpleRabbitListenerContainerFactory factory =
                new SimpleRabbitListenerContainerFactory();
        factory.setConnectionFactory(connectionFactory);
        factory.setMessageConverter(jacksonConverter());
        return factory;
    }

    @Bean
    public MessageConverter jacksonConverter() {
        return new Jackson2JsonMessageConverter(objectMapper);
    }

}

================
File: src/main/java/com/dms/search/consumer/EventConsumer.java
================
package com.dms.search.consumer;

import com.dms.search.enums.EventType;
import com.dms.search.model.DocumentInformation;
import com.dms.search.model.SyncEventRequest;
import com.dms.search.repository.DocumentRepository;
import com.dms.search.service.DocumentProcessService;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.amqp.rabbit.annotation.RabbitListener;
import org.springframework.stereotype.Component;

import java.util.Optional;

@Component
@Slf4j
@RequiredArgsConstructor
public class EventConsumer {
    private final DocumentService documentService;
    private final DocumentRepository documentRepository;

    @RabbitListener(queues = "${rabbitmq.queues.document-sync}")
    public void consumeSyncEvent(SyncEventRequest syncEventRequest) {
        log.info("Consumed event: [ID: {}, Type: {}]",
                syncEventRequest.getEventId(),
                syncEventRequest.getSubject());

        try {
            EventType eventType = EventType.valueOf(syncEventRequest.getSubject());
            switch (eventType) {
                case DELETE_EVENT -> handleDeleteEvent(syncEventRequest);
                case UPDATE_EVENT -> handleUpdateEvent(syncEventRequest);
                case SYNC_EVENT -> handleSyncEvent(syncEventRequest);
                default -> log.warn("Unhandled event type: {}", eventType);
            }
        } catch (IllegalArgumentException e) {
            log.error("Invalid event type: {}", syncEventRequest.getSubject());
        } catch (Exception e) {
            log.error("Error processing event: {}", syncEventRequest.getEventId(), e);
        }
    }

    private void handleDeleteEvent(SyncEventRequest request) {
        log.info("Processing delete event for document: {}", request.getDocumentId());
        documentService.deleteDocumentFromIndex(request.getDocumentId());
    }

    private void handleUpdateEvent(SyncEventRequest request) {
        log.info("Processing update event for document: {}", request.getDocumentId());
        findAndProcessDocument(request);
    }

    private void handleSyncEvent(SyncEventRequest request) {
        log.info("Processing sync event for document: {}", request.getDocumentId());
        findAndProcessDocument(request);
    }

    private void findAndProcessDocument(SyncEventRequest request) {
        Optional<DocumentInformation> documentOpt = documentRepository.findByIdAndUserId(
                request.getDocumentId(),
                request.getUserId()
        );

        documentOpt.ifPresentOrElse(
                document -> {
                    if (document.isDeleted()) {
                        log.info("Document {} is marked as deleted, removing from index", document.getId());
                        documentService.deleteDocumentFromIndex(document.getId());
                    } else {
                        log.info("Indexing document: {}", document.getId());
                        documentService.indexDocument(document);
                    }
                },
                () -> log.warn("Document not found: {}", request.getDocumentId())
        );
    }
}

================
File: src/main/java/com/dms/search/dto/DocumentContent.java
================
package com.dms.search.dto;

import java.util.Map;


public record DocumentContent(
        String content,
        Map<String, String> metadata
) {
}

================
File: src/main/java/com/dms/search/dto/ErrorResponse.java
================
package com.dms.search.dto;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
class ErrorResponse {
    private int status;
    private String message;
    private String details;

    public ErrorResponse(int status, String message) {
        this.status = status;
        this.message = message;
    }
}

================
File: src/main/java/com/dms/search/dto/ExtractedText.java
================
package com.dms.search.dto;

public record ExtractedText(String text, boolean usedOcr) {

}

================
File: src/main/java/com/dms/search/dto/UserDto.java
================
package com.dms.search.dto;

import lombok.Data;
import java.util.UUID;

@Data
public class UserDto {
    private UUID userId;
    private String username;
    private String email;
}

================
File: src/main/java/com/dms/search/elasticsearch/repository/DocumentIndexRepository.java
================
package com.dms.search.elasticsearch.repository;

import com.dms.search.elasticsearch.DocumentIndex;
import org.springframework.data.elasticsearch.repository.ElasticsearchRepository;

public interface DocumentIndexRepository extends ElasticsearchRepository<DocumentIndex, String> {
}

================
File: src/main/java/com/dms/search/elasticsearch/DocumentIndex.java
================
package com.dms.search.elasticsearch;

import com.dms.search.enums.DocumentType;
import lombok.Builder;
import lombok.Data;
import org.springframework.data.annotation.Id;
import org.springframework.data.elasticsearch.annotations.*;

import java.util.Date;
import java.util.Map;
import java.util.Set;

@Builder
@Data
@Document(indexName = "documents")
public class DocumentIndex {
    @Id
    private String id;

    @MultiField(
            mainField = @Field(type = FieldType.Text),
            otherFields = {
                    @InnerField(suffix = "raw", type = FieldType.Keyword)
            }
    )
    private String filename;

    @Field(type = FieldType.Text)
    private String content;

    @Field(type = FieldType.Keyword)
    private String userId;

    @Field(type = FieldType.Long)
    private Long fileSize;

    @Field(type = FieldType.Keyword)
    private String mimeType;

    @Field(type = FieldType.Keyword)
    private DocumentType documentType;

    @Field(type = FieldType.Keyword)
    private String major;

    @Field(type = FieldType.Keyword)
    private String courseCode;

    @Field(type = FieldType.Keyword)
    private String courseLevel;

    @Field(type = FieldType.Keyword)
    private String category;

    @Field(type = FieldType.Keyword)
    private Set<String> tags;

    @Field(type = FieldType.Object)
    private Map<String, String> extractedMetadata;

    @Field(type = FieldType.Boolean)
    private boolean isShared;

    @Field(type = FieldType.Date)
    private Date createdAt;
}

================
File: src/main/java/com/dms/search/enums/DocumentType.java
================
package com.dms.search.enums;

import com.dms.search.exception.UnsupportedDocumentTypeException;
import lombok.AllArgsConstructor;
import lombok.Getter;

@AllArgsConstructor
@Getter
public enum DocumentType {
    PDF("application/pdf", "PDF Document"),
    WORD("application/msword", "Word Document (DOC)"),
    WORD_DOCX("application/vnd.openxmlformats-officedocument.wordprocessingml.document", "Word Document (DOCX)"),
    EXCEL("application/vnd.ms-excel", "Excel Document (XLS)"),
    EXCEL_XLSX("application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", "Excel Document (XLSX)"),
    POWERPOINT("application/vnd.ms-powerpoint", "PowerPoint (PPT)"),
    POWERPOINT_PPTX("application/vnd.openxmlformats-officedocument.presentationml.presentation", "PowerPoint (PPTX)"),
    TEXT_PLAIN("text/plain", "Text Document"),
    RTF("application/rtf", "Rich Text Format"),
    CSV("text/csv", "CSV Document"),
    XML("application/xml", "XML Document"),
    JSON("application/json", "JSON Document");

    private final String mimeType;
    private final String displayName;

    public static DocumentType fromMimeType(String mimeType) {
        for (DocumentType type : values()) {
            if (type.getMimeType().equals(mimeType)) {
                return type;
            }
        }
        throw new UnsupportedDocumentTypeException("Unsupported document type: " + mimeType);
    }

    public static boolean isSupportedMimeType(String mimeType) {
        for (DocumentType type : values()) {
            if (type.getMimeType().equals(mimeType)) {
                return true;
            }
        }
        return false;
    }
}

================
File: src/main/java/com/dms/search/enums/EventType.java
================
package com.dms.search.enums;

public enum EventType {
    SYNC_EVENT,
    UPDATE_EVENT,
    DELETE_EVENT
}

================
File: src/main/java/com/dms/search/exception/InvalidDocumentException.java
================
package com.dms.search.exception;

public class InvalidDocumentException extends RuntimeException {
    public InvalidDocumentException(String message) {
        super(message);
    }
}

================
File: src/main/java/com/dms/search/exception/UnsupportedDocumentTypeException.java
================
package com.dms.search.exception;

public class UnsupportedDocumentTypeException extends RuntimeException {
    public UnsupportedDocumentTypeException(String message) {
        super(message);
    }
}

================
File: src/main/java/com/dms/search/model/DocumentInformation.java
================
package com.dms.search.model;

import com.dms.search.enums.DocumentType;
import lombok.Builder;
import lombok.Data;
import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.index.Indexed;
import org.springframework.data.mongodb.core.mapping.Document;
import org.springframework.data.mongodb.core.mapping.Field;

import java.util.Date;
import java.util.Map;
import java.util.Set;

@Data
@Builder
@Document(collection = "documents")
public class DocumentInformation {
    @Id
    private String id;

    @Field("filename")
    private String filename;

    @Field("original_filename")
    private String originalFilename;

    @Field("file_path")
    private String filePath;

    @Field("file_size")
    private Long fileSize;

    @Field("mime_type")
    private String mimeType;

    @Indexed
    @Field("document_type")
    private DocumentType documentType;

    @Field("content")
    private String content;

    @Indexed
    @Field("major")
    private String major;

    @Indexed
    @Field("course_code")
    private String courseCode;

    @Indexed
    @Field("course_level")
    private String courseLevel;

    @Indexed
    @Field("category")
    private String category;

    @Indexed
    @Field("tags")
    private Set<String> tags;

    @Field("extracted_metadata")
    private Map<String, String> extractedMetadata;

    @Field("user_id")
    private String userId;

    @Field("is_shared")
    private boolean isShared;

    @Field("deleted")
    private boolean deleted;

    @Field("created_at")
    private Date createdAt;

    @Field("updated_at")
    private Date updatedAt;

    @Field("created_by")
    private String createdBy;

    @Field("updated_by")
    private String updatedBy;
}

================
File: src/main/java/com/dms/search/model/EventRequest.java
================
package com.dms.search.model;

import com.fasterxml.jackson.annotation.JsonFormat;
import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.experimental.SuperBuilder;

import java.time.LocalDateTime;

@NoArgsConstructor
@SuperBuilder
@Data
public class EventRequest {
    private String userId;

    private String eventId;

    private String subject;

    @JsonFormat(pattern = "yyyy-MM-dd'T'HH:mm:ss.SSS'ZZZZZ'")
    private LocalDateTime triggerAt;
}

================
File: src/main/java/com/dms/search/model/SyncEventRequest.java
================
package com.dms.search.model;

import lombok.Getter;
import lombok.NoArgsConstructor;
import lombok.Setter;
import lombok.experimental.SuperBuilder;


@SuperBuilder
@NoArgsConstructor
@Getter
@Setter
public class SyncEventRequest extends EventRequest {
    private String documentId;
}

================
File: src/main/java/com/dms/search/repository/DocumentRepository.java
================
package com.dms.search.repository;

import com.dms.search.model.DocumentInformation;
import org.springframework.data.mongodb.repository.MongoRepository;
import org.springframework.data.mongodb.repository.Query;

import java.util.List;
import java.util.Optional;

public interface DocumentRepository extends MongoRepository<DocumentInformation, String> {
    Optional<DocumentInformation> findByIdAndUserId(String id, String userId);

    @Query("{'tags': {'$regex': ?0, '$options': 'i'}}")
    List<String> findDistinctTagsByPattern(String pattern);

    @Query(value = "{'tags': {'$exists': true}}", fields = "{'tags': 1}")
    List<DocumentInformation> findAllTags();
}

================
File: src/main/java/com/dms/search/service/ContentExtractorService.java
================
package com.dms.search.service;

import com.dms.search.dto.DocumentContent;
import com.dms.search.dto.ExtractedText;
import jakarta.annotation.PreDestroy;
import lombok.extern.slf4j.Slf4j;
import net.sourceforge.tess4j.TesseractException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.ocr.TesseractOCRConfig;
import org.apache.tika.sax.ToTextContentHandler;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;
import org.springframework.util.unit.DataSize;
import org.xml.sax.ContentHandler;
import org.xml.sax.helpers.DefaultHandler;

import java.io.BufferedInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.StringWriter;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;

@Slf4j
@Service
public class ContentExtractorService {
    private final AutoDetectParser parser;
    private final ExecutorService executorService;
    private final SmartPdfExtractor smartPdfExtractor;
    private final OcrLargeFileProcessor ocrLargeFileProcessor;

    private final AtomicInteger threadCounter = new AtomicInteger(1);
    private final LargeFileProcessor largeFileProcessor;

    @Value("${app.document.max-size-threshold-mb}")
    private DataSize maxSizeThreshold;

    public ContentExtractorService(SmartPdfExtractor smartPdfExtractor, OcrLargeFileProcessor ocrLargeFileProcessor, LargeFileProcessor largeFileProcessor) {
        this.smartPdfExtractor = smartPdfExtractor;
        this.ocrLargeFileProcessor = ocrLargeFileProcessor;
        this.parser = new AutoDetectParser();
        this.executorService = Executors.newFixedThreadPool(
                Runtime.getRuntime().availableProcessors(),
                r -> {
                    Thread thread = new Thread(r);
                    thread.setName("content-extractor-" + threadCounter.getAndIncrement());
                    thread.setDaemon(true);
                    return thread;
                }
        );
        this.largeFileProcessor = largeFileProcessor;
    }

    public DocumentContent extractContent(Path filePath) {
        try {
            String mimeType = Files.probeContentType(filePath);
            if (mimeType == null) {
                log.warn("Could not determine mime type for file: {}", filePath);
                return new DocumentContent("", new HashMap<>());
            }

            // Initialize metadata map
            Map<String, String> metadata = new HashMap<>();

            String extractedText;
            if ("application/pdf".equals(mimeType)) {
                extractedText = handlePdfExtraction(filePath, metadata);
                return new DocumentContent(extractedText, metadata);
            } else {
                if (Files.size(filePath) > maxSizeThreshold.toBytes()) {
                    // Use new large file handler for non-PDF large files
                    CompletableFuture<String> future = largeFileProcessor.processLargeFile(filePath);
                    String content = future.get(30, TimeUnit.MINUTES);
                    return new DocumentContent(content, metadata);
                } else {
                    // Handle other file types with existing logic
                    CompletableFuture<String> contentFuture = CompletableFuture.supplyAsync(() ->
                            extractTextContent(filePath), executorService);

                    try {
                        // Wait for the result with a timeout
                        String result = contentFuture.get(5, TimeUnit.MINUTES); // Add appropriate timeout
                        return new DocumentContent(result, metadata);
                    } catch (InterruptedException | ExecutionException | TimeoutException e) {
                        log.error("Error extracting content from file: {}", filePath, e);
                        return new DocumentContent("", metadata);
                    }
                }

            }
        } catch (Exception e) {
            log.error("Error extracting content from file: {}", filePath, e);
            return new DocumentContent("", new HashMap<>());
        }
    }

    private String handlePdfExtraction(Path filePath, Map<String, String> metadata) throws IOException, TesseractException {
        long fileSizeInMb = Files.size(filePath) / (1024 * 1024);
        metadata.put("File-Size-MB", String.valueOf(fileSizeInMb));

        if (fileSizeInMb > maxSizeThreshold.toBytes()) {
            log.info("Large PDF detected ({}MB). Using chunked processing.", fileSizeInMb);
            metadata.put("Processing-Method", "chunked");
            return ocrLargeFileProcessor.processLargePdf(filePath);
        }

        log.info("Processing regular PDF ({}MB)", fileSizeInMb);
        ExtractedText result = smartPdfExtractor.extractText(filePath);
        metadata.put("Processing-Method", result.usedOcr() ? "ocr" : "direct");
        metadata.put("Used-OCR", String.valueOf(result.usedOcr()));

        return result.text();
    }

    private String extractTextContent(Path filePath) {
        try (InputStream input = new BufferedInputStream(Files.newInputStream(filePath))) {
            StringWriter writer = new StringWriter();
            ContentHandler handler = new ToTextContentHandler(writer);
            Metadata metadata = new Metadata();
            ParseContext context = new ParseContext();

            // Configure Tesseract to skip OCR for non-PDF files
            TesseractOCRConfig config = new TesseractOCRConfig();
            config.setSkipOcr(true);
            context.set(TesseractOCRConfig.class, config);

            parser.parse(input, handler, metadata, context);
            return writer.toString();
        } catch (Exception e) {
            log.error("Error extracting text content", e);
            return "";
        }
    }

    private Map<String, String> extractMetadata(Path filePath) {
        try (InputStream input = new BufferedInputStream(Files.newInputStream(filePath))) {
            Metadata metadata = new Metadata();
            ParseContext context = new ParseContext();
            ContentHandler handler = new DefaultHandler();

            parser.parse(input, handler, metadata, context);

            return Arrays.stream(metadata.names())
                    .filter(this::isImportantMetadata)
                    .collect(Collectors.toMap(
                            name -> name,
                            metadata::get,
                            (v1, v2) -> v1
                    ));

        } catch (Exception e) {
            log.error("Error extracting metadata", e);
            return new HashMap<>();
        }
    }

    private boolean isImportantMetadata(String name) {
        return Set.of(
                "Content-Type",
                "Last-Modified",
                "Creation-Date",
                "Author",
                "Page-Count",
                "Word-Count"
        ).contains(name);
    }

    @PreDestroy
    public void shutdown() {
        executorService.shutdown();
        try {
            if (!executorService.awaitTermination(60, TimeUnit.SECONDS)) {
                executorService.shutdownNow();
            }
        } catch (InterruptedException e) {
            executorService.shutdownNow();
            Thread.currentThread().interrupt();
        }
    }
}

================
File: src/main/java/com/dms/search/service/DocumentService.java
================
package com.dms.search.service;

import com.dms.search.elasticsearch.DocumentIndex;
import com.dms.search.elasticsearch.repository.DocumentIndexRepository;
import com.dms.search.model.DocumentInformation;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

@Service
@RequiredArgsConstructor
@Slf4j
public class DocumentService {
    private final DocumentIndexRepository documentIndexRepository;

    public void indexDocument(DocumentInformation document) {
        try {
            DocumentIndex documentIndex = DocumentIndex.builder()
                    .id(document.getId())
                    .filename(document.getOriginalFilename())
                    .content(document.getContent())
                    .userId(document.getUserId())
                    .mimeType(document.getMimeType())
                    .documentType(document.getDocumentType())
                    .major(document.getMajor())
                    .courseCode(document.getCourseCode())
                    .courseLevel(document.getCourseLevel())
                    .category(document.getCategory())
                    .tags(document.getTags())
                    .fileSize(document.getFileSize())
                    .isShared(document.isShared())
                    .createdAt(document.getCreatedAt())
                    .build();

            documentIndexRepository.save(documentIndex);
            log.info("Successfully indexed document: {}", document.getId());
        } catch (Exception e) {
            log.error("Error indexing document: {}", document.getId(), e);
            throw new RuntimeException("Failed to index document", e);
        }
    }

    public void deleteDocumentFromIndex(String documentId) {
        try {
            documentIndexRepository.deleteById(documentId);
            log.info("Successfully deleted document {} from index", documentId);
        } catch (Exception e) {
            log.error("Error deleting document {} from index", documentId, e);
            throw new RuntimeException("Failed to delete document from index", e);
        }
    }
}

================
File: src/main/java/com/dms/search/service/LargeFileProcessor.java
================
package com.dms.search.service;

import lombok.extern.slf4j.Slf4j;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.sax.ToTextContentHandler;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.io.*;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;

@Slf4j
@Service
public class LargeFileProcessor {
    private final ExecutorService executorService;
    private final ConcurrentHashMap<String, CompletableFuture<String>> processingTasks;
    private final AtomicInteger threadCounter = new AtomicInteger(1);

    @Value("${app.document.chunk-size-mb:5}")
    private int chunkSizeMB;

    public LargeFileProcessor() {
        this.executorService = Executors.newFixedThreadPool(
                Runtime.getRuntime().availableProcessors(),
                r -> {
                    Thread thread = new Thread(r);
                    thread.setName("large-file-handler-" + threadCounter.getAndIncrement());
                    thread.setDaemon(true);
                    return thread;
                }
        );
        this.processingTasks = new ConcurrentHashMap<>();
    }

    public CompletableFuture<String> processLargeFile(Path filePath) {
        String fileId = filePath.getFileName().toString();

        return processingTasks.computeIfAbsent(fileId, id -> {
            CompletableFuture<String> future = new CompletableFuture<>();

            CompletableFuture.runAsync(() -> {
                try {
                    String result = processFileInChunks(filePath);
                    future.complete(result);
                } catch (Exception e) {
                    log.error("Error processing file: {}", filePath, e);
                    future.completeExceptionally(e);
                } finally {
                    processingTasks.remove(fileId);
                }
            }, executorService);

            return future;
        });
    }

    private String processFileInChunks(Path filePath) throws IOException {
        try (InputStream inputStream = new BufferedInputStream(Files.newInputStream(filePath))) {
            long fileSize = Files.size(filePath);
            int chunkSize = chunkSizeMB * 1024 * 1024; // Convert MB to bytes
            long totalChunks = (fileSize + chunkSize - 1) / chunkSize;

            StringBuilder result = new StringBuilder();
            byte[] buffer = new byte[chunkSize];
            int chunkNumber = 0;
            int bytesRead;

            Parser parser = new AutoDetectParser();
            ParseContext context = new ParseContext();

            while ((bytesRead = inputStream.read(buffer)) != -1) {
                chunkNumber++;
                log.info("Processing chunk {}/{} for file: {}",
                        chunkNumber, totalChunks, filePath.getFileName());

                String chunkText = processChunk(buffer, bytesRead, parser, context);
                result.append(chunkText);

                // Add progress tracking
                double progress = (chunkNumber * 100.0) / totalChunks;
                log.info("Progress: {}% for file: {}",
                        String.format("%.2f", progress), filePath.getFileName());
            }

            return result.toString();
        }
    }

    private String processChunk(byte[] buffer, int bytesRead, Parser parser, ParseContext context)
            throws IOException {
        try {
            ToTextContentHandler handler = new ToTextContentHandler();
            Metadata metadata = new Metadata();

            parser.parse(
                    new ByteArrayInputStream(buffer, 0, bytesRead),
                    handler,
                    metadata,
                    context
            );

            return handler.toString();
        } catch (Exception e) {
            log.error("Error processing chunk", e);
            throw new IOException("Failed to process file chunk", e);
        }
    }

    public void cancelProcessing(String fileId) {
        CompletableFuture<String> task = processingTasks.get(fileId);
        if (task != null) {
            task.cancel(true);
            processingTasks.remove(fileId);
        }
    }

    public double getProcessingProgress(String fileId) {
        // Implementation for progress tracking
        // This could be enhanced with a more sophisticated progress tracking mechanism
        return processingTasks.containsKey(fileId) ? -1 : 100;
    }
}

================
File: src/main/java/com/dms/search/service/OcrLargeFileProcessor.java
================
package com.dms.search.service;

import jakarta.annotation.PostConstruct;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import net.sourceforge.tess4j.TesseractException;
import org.apache.pdfbox.pdmodel.PDDocument;
import org.apache.pdfbox.rendering.PDFRenderer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import javax.imageio.ImageIO;
import java.awt.image.BufferedImage;
import java.io.File;
import java.io.IOException;
import java.nio.file.Path;
import java.util.ArrayList;
import java.util.List;
import java.util.UUID;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.atomic.AtomicInteger;

@Service
@Slf4j
@RequiredArgsConstructor
public class OcrLargeFileProcessor {

    private final OcrService ocrService;
    private ExecutorService executorService;

    @Value("${app.ocr.chunk-size:10}")
    private int chunkSize;

    @Value("${app.ocr.temp-dir:/tmp/ocr}")
    private String tempDir;

    @Value("${app.ocr.max-threads:4}")
    private int maxThreads;

    @PostConstruct
    private void initialize() {
        this.executorService = Executors.newFixedThreadPool(maxThreads);
        log.info("Initialized thread pool with {} threads", maxThreads);
    }

    public String processLargePdf(Path pdfPath) throws IOException, TesseractException {
        File tempDirectory = createTempDirectory();
        AtomicInteger processedPages = new AtomicInteger(0);
        List<CompletableFuture<String>> futures = new ArrayList<>();

        try (PDDocument document = PDDocument.load(pdfPath.toFile())) {
            int totalPages = document.getNumberOfPages();
            PDFRenderer renderer = new PDFRenderer(document);

            // Try text extraction first
            String extractedText = ocrService.extractTextFromPdf(pdfPath);
            if (isTextSufficient(extractedText)) {
                return extractedText;
            }

            // Process in chunks
            for (int chunkStart = 0; chunkStart < totalPages; chunkStart += chunkSize) {
                int chunkEnd = Math.min(chunkStart + chunkSize, totalPages);
                futures.add(processChunk(renderer, chunkStart, chunkEnd, tempDirectory, processedPages, totalPages));
            }

            // Combine results
            StringBuilder combinedText = new StringBuilder();
            for (CompletableFuture<String> future : futures) {
                try {
                    String chunkText = future.get();
                    combinedText.append(chunkText).append("\n");
                } catch (Exception e) {
                    log.error("Error processing PDF chunk", e);
                }
            }

            return combinedText.toString();
        } finally {
            cleanupTempDirectory(tempDirectory);
        }
    }

    private CompletableFuture<String> processChunk(
            PDFRenderer renderer,
            int startPage,
            int endPage,
            File tempDirectory,
            AtomicInteger processedPages,
            int totalPages) {

        return CompletableFuture.supplyAsync(() -> {
            StringBuilder chunkText = new StringBuilder();

            try {
                for (int pageNum = startPage; pageNum < endPage; pageNum++) {
                    BufferedImage image = renderer.renderImageWithDPI(pageNum, 300);

                    // Preprocess the image
                    image = ocrService.preprocessImage(image);

                    // Save image temporarily
                    File tempImage = new File(tempDirectory, UUID.randomUUID() + ".png");
                    ImageIO.write(image, "PNG", tempImage);

                    // Process with OCR using the public method
                    String pageText = ocrService.performOcrOnImage(image);
                    chunkText.append(pageText).append("\n");

                    tempImage.delete();

                    int completed = processedPages.incrementAndGet();
                    log.info("Processed page {} of {} ({}%)",
                            completed, totalPages,
                            (completed * 100) / totalPages);

                    image.flush();
                }

                return chunkText.toString();

            } catch (Exception e) {
                log.error("Error processing pages {} to {}", startPage, endPage, e);
                throw new RuntimeException(e);
            }
        });
    }

    private File createTempDirectory() throws IOException {
        File tempDirectory = new File(tempDir, UUID.randomUUID().toString());
        if (!tempDirectory.exists() && !tempDirectory.mkdirs()) {
            throw new IOException("Failed to create temp directory: " + tempDirectory);
        }
        return tempDirectory;
    }

    private void cleanupTempDirectory(File directory) {
        if (directory != null && directory.exists()) {
            File[] files = directory.listFiles();
            if (files != null) {
                for (File file : files) {
                    if (!file.delete()) {
                        log.warn("Failed to delete temporary file: {}", file.getAbsolutePath());
                    }
                }
            }
            if (!directory.delete()) {
                log.warn("Failed to delete temporary directory: {}", directory.getAbsolutePath());
            }
        }
    }

    private boolean isTextSufficient(String text) {
        if (text == null || text.isEmpty()) return false;

        int minChars = 100;
        int recognizableChars = text.replaceAll("[^a-zA-Z0-9\\s.,;:!?()\\[\\]{}\"'`-]", "").length();

        return recognizableChars > minChars;
    }

    public void shutdown() {
        if (executorService != null && !executorService.isShutdown()) {
            executorService.shutdown();
        }
    }
}

================
File: src/main/java/com/dms/search/service/OcrService.java
================
package com.dms.search.service;

import jakarta.annotation.PostConstruct;
import net.sourceforge.tess4j.Tesseract;
import net.sourceforge.tess4j.TesseractException;
import org.apache.pdfbox.pdmodel.PDDocument;
import org.apache.pdfbox.rendering.ImageType;
import org.apache.pdfbox.rendering.PDFRenderer;
import org.apache.pdfbox.text.PDFTextStripper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.awt.image.BufferedImage;
import java.io.IOException;
import java.nio.file.Path;

@Service
public class OcrService {
    private static final Logger logger = LoggerFactory.getLogger(OcrService.class);
    private final Tesseract tesseract;

    @Value("${app.ocr.data-path}")
    private String tessdataPath;

    @Value("${app.ocr.minimum-text-length}")
    private int minimumTextLength;

    @Value("${app.ocr.dpi}")
    private float dpi;

    @Value("${app.ocr.image-type}")
    private String imageType;

    public OcrService() {
        tesseract = new Tesseract();
    }

    @PostConstruct
    private void initialize() {
        tesseract.setDatapath(tessdataPath);
        tesseract.setLanguage("eng+vie");

        // Configure Tesseract for better accuracy
        tesseract.setPageSegMode(1);  // Automatic page segmentation with OSD
        tesseract.setOcrEngineMode(1); // Neural net LSTM engine only

        // Additional settings for better performance
        tesseract.setVariable("textord_max_iterations", "5");
    }

    public String extractTextFromPdf(Path pdfPath) throws IOException, TesseractException {
        StringBuilder extractedText = new StringBuilder();
        boolean usedOcr = false;

        try (PDDocument document = PDDocument.load(pdfPath.toFile())) {
            PDFTextStripper textStripper = new PDFTextStripper();
            String pdfText = textStripper.getText(document);

            if (pdfText.trim().length() > minimumTextLength) {
                return pdfText;
            }

            PDFRenderer pdfRenderer = new PDFRenderer(document);
            int pageCount = document.getNumberOfPages();

            for (int page = 0; page < pageCount; page++) {
                BufferedImage image = pdfRenderer.renderImageWithDPI(
                        page,
                        dpi,
                        imageType.equals("RGB") ? ImageType.RGB : ImageType.BINARY
                );

                image = preprocessImage(image);
                String pageText = performOcrOnImage(image);
                if (pageText != null && !pageText.trim().isEmpty()) {
                    extractedText.append(pageText).append("\n");
                    usedOcr = true;
                }
            }
        }

        logger.info("PDF processing completed. Used OCR: {}", usedOcr);
        return extractedText.toString();
    }

    // Made public for use by LargeFileProcessor
    public String performOcrOnImage(BufferedImage image) throws TesseractException {
        try {
            return tesseract.doOCR(image);
        } catch (TesseractException e) {
            logger.error("OCR processing error", e);
            if (e.getMessage().contains("osd.traineddata")) {
                tesseract.setPageSegMode(3); // Fully automatic page segmentation, but no OSD
                return tesseract.doOCR(image);
            }
            throw e;
        }
    }

    // Also made public for potential external preprocessing
    public BufferedImage preprocessImage(BufferedImage image) {
        // Add any image preprocessing here if needed
        return image;
    }

    public boolean isImageBasedPdf(Path pdfPath) throws IOException {
        try (PDDocument document = PDDocument.load(pdfPath.toFile())) {
            PDFTextStripper textStripper = new PDFTextStripper();
            String pdfText = textStripper.getText(document);
            return pdfText.trim().length() < minimumTextLength;
        }
    }
}

================
File: src/main/java/com/dms/search/service/SmartPdfExtractor.java
================
package com.dms.search.service;

import com.dms.search.dto.ExtractedText;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import net.sourceforge.tess4j.TesseractException;
import org.apache.pdfbox.pdmodel.PDDocument;
import org.apache.pdfbox.text.PDFTextStripper;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.io.IOException;
import java.nio.file.Path;


@Service
@RequiredArgsConstructor
@Slf4j
public class SmartPdfExtractor {

    private final OcrService ocrService;

    @Value("${app.pdf.quality-threshold:0.8}")
    private double qualityThreshold;

    @Value("${app.pdf.min-text-density:0.01}")
    private double minTextDensity;

    public ExtractedText extractText(Path pdfPath) throws IOException, TesseractException {
        try (PDDocument document = PDDocument.load(pdfPath.toFile())) {
            PDFTextStripper textStripper = new PDFTextStripper();
            String pdfText = textStripper.getText(document);

            // Calculate text quality metrics
            double textDensity = calculateTextDensity(pdfText, document.getNumberOfPages());
            double textQuality = assessTextQuality(pdfText);

            boolean useOcr = shouldUseOcr(textDensity, textQuality);

            if (!useOcr) {
                log.info("Using PDFTextStripper - Density: {}, Quality: {}", textDensity, textQuality);
                return new ExtractedText(pdfText, false);
            }

            // Use OCR if needed
            log.info("Using OCR - Low text metrics - Density: {}, Quality: {}", textDensity, textQuality);
            String ocrText = ocrService.extractTextFromPdf(pdfPath);
            return new ExtractedText(ocrText, true);
        }
    }

    private double calculateTextDensity(String text, int pageCount) {
        if (pageCount == 0) return 0;

        // Calculate characters per page
        double charsPerPage = (double) text.length() / pageCount;

        // Normalize against expected minimum chars per page
        final double expectedMinCharsPerPage = 250; // Configurable
        return Math.min(charsPerPage / expectedMinCharsPerPage, 1.0);
    }

    private double assessTextQuality(String text) {
        if (text == null || text.isEmpty()) return 0;

        // Count recognizable characters vs total
        int totalChars = text.length();
        int recognizableChars = text.replaceAll("[^a-zA-Z0-9\\s.,;:!?()\\[\\]{}\"'`-]", "").length();

        return (double) recognizableChars / totalChars;
    }

    private boolean shouldUseOcr(double textDensity, double textQuality) {
        return textDensity < minTextDensity || textQuality < qualityThreshold;
    }
}

================
File: src/main/java/com/dms/search/SearchServiceApplication.java
================
package com.dms.search;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class SearchServiceApplication {

    public static void main(String[] args) {
        SpringApplication.run(SearchServiceApplication.class, args);
    }

}

================
File: src/main/resources/application-local.yml
================
app:
  eureka:
    url: http://localhost:8081
  document:
    storage:
      path: ./document/storage
    max-size-threshold-mb: 25MB
    max-size-mb: 50MB
    thumbnail:
      width: 300
      height: 200
  ocr:
    data-path: /usr/share/tesseract-ocr/5/tessdata
    minimum-text-length: 50  # Minimum text length to consider a PDF as text-based
    chunk-size: 10
    max-threads: 4
    temp-dir: /tmp/ocr
    dpi: 300
    image-type: RGB
  pdf:
    quality-threshold: 0.8
    min-text-density: 0.01

spring:
  security:
    oauth2:
      resourceserver:
        jwt:
          issuer-uri: http://localhost:8082
          jwk-set-uri: http://localhost:8082/.well-known/jwks.json
  data:
    mongodb:
      authentication-database: admin
      username: mongodb
      password: mongodbpw
      database: dms
      port: 27017
      host: localhost


elasticsearch:
  host: localhost
  port: 9200
  username: elastic  # Default username for Elasticsearch 8.x
  password: "NX9DYn+_4WSXRmHwymTo"
  # Updated index settings for 8.x
  index:
    settings:
      number_of_shards: 1
      number_of_replicas: 0
      analysis:
        analyzer:
          custom_analyzer:
            type: custom
            tokenizer: standard
            filter:
              - lowercase
              - asciifolding
              - word_delimiter_graph
    mapping:
      total_fields:
        limit: 2000

# RabbitMQ
rabbitmq:
  exchanges:
    internal: internal.exchange
  queues:
    document-sync: document-sync.queue
  routing-keys:
    internal-document-sync: internal.document-sync.routing-key

# Eureka Client
eureka:
  client:
    register-with-eureka: true
    fetch-registry: true
    service-url:
      defaultZone: ${app.eureka.url}/eureka
    enabled: true
  instance:
    preferIpAddress: true
    hostname: localhost
    lease-renewal-interval-in-seconds: 5
    lease-expiration-duration-in-seconds: 10

================
File: src/main/resources/application.yml
================
server:
  port: 8084
  error:
    include-message: always

logging:
  level:
    org:
      springframework:
        web: DEBUG
        security: DEBUG
        data:
          elasticsearch: DEBUG
      elasticsearch: DEBUG

spring:
  application:
    name: processor-service
  profiles:
    active: local

================
File: .gitattributes
================
/mvnw text eol=lf
*.cmd text eol=crlf

================
File: .gitignore
================
HELP.md
target/
!.mvn/wrapper/maven-wrapper.jar
!**/src/main/**/target/
!**/src/test/**/target/

### STS ###
.apt_generated
.classpath
.factorypath
.project
.settings
.springBeans
.sts4-cache

### IntelliJ IDEA ###
.idea
*.iws
*.iml
*.ipr

### NetBeans ###
/nbproject/private/
/nbbuild/
/dist/
/nbdist/
/.nb-gradle/
build/
!**/src/main/**/build/
!**/src/test/**/build/

### VS Code ###
.vscode/

================
File: pom.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.2.8</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
    <groupId>com.dms</groupId>
    <artifactId>processor-service</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>processor-service</name>
    <description>processor-service</description>

    <properties>
        <java.version>17</java.version>
        <spring-cloud.version>2023.0.0</spring-cloud.version>
        <elasticsearch.version>8.8.1</elasticsearch.version>
    </properties>
    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-actuator</artifactId>
        </dependency>

        <!-- Elasticsearch -->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-elasticsearch</artifactId>
        </dependency>

        <!-- MongoDB -->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-mongodb</artifactId>
        </dependency>

        <!-- RabbitMQ -->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-amqp</artifactId>
        </dependency>

        <!-- Tesseract OCR -->
        <dependency>
            <groupId>net.sourceforge.tess4j</groupId>
            <artifactId>tess4j</artifactId>
            <version>5.8.0</version>
        </dependency>

        <!-- PDF Box for PDF processing -->
        <dependency>
            <groupId>org.apache.pdfbox</groupId>
            <artifactId>pdfbox</artifactId>
            <version>2.0.29</version>
        </dependency>

        <dependency>
            <groupId>org.apache.tika</groupId>
            <artifactId>tika-core</artifactId>
            <version>2.9.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.tika</groupId>
            <artifactId>tika-parsers-standard-package</artifactId>
            <version>2.9.1</version>
        </dependency>

        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>org.springframework.cloud</groupId>
                <artifactId>spring-cloud-dependencies</artifactId>
                <version>${spring-cloud.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
        </dependencies>
    </dependencyManagement>

    <packaging>jar</packaging>
    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
    <repositories>
        <repository>
            <id>spring-milestones</id>
            <name>Spring Milestones</name>
            <url>https://repo.spring.io/milestone</url>
            <snapshots>
                <enabled>false</enabled>
            </snapshots>
        </repository>
        <repository>
            <id>netflix-candidates</id>
            <name>Netflix Candidates</name>
            <url>https://artifactory-oss.prod.netflix.net/artifactory/maven-oss-candidates</url>
            <snapshots>
                <enabled>false</enabled>
            </snapshots>
        </repository>
    </repositories>
</project>

================
File: README.md
================
docker run -d --hostname rabbitmq --name rabbitmq -p 5672:5672 15672:15672 rabbitmq:3-management-alpine  

docker run -p 9200:9200 \
--name elasticsearch \
-e "discovery.type=single-node" \
-e "xpack.security.enabled=false" \
-e "ES_JAVA_OPTS=-Xms512m -Xmx512m" \
-d docker.elastic.co/elasticsearch/elasticsearch:8.8.1


Add authentication
```shell
curl -X GET "localhost:9200/documents/_count" \
-H "Content-Type: application/json" \
-u "elastic:NX9DYn+_4WSXRmHwymTo"
```


curl -X DELETE "localhost:9200/documents"


curl -X GET "localhost:9200/documents/_search" -H "Content-Type: application/json" -d '{}'

curl -X PUT "localhost:9200/documents" -H "Content-Type: application/json" -d @mapping.json

To get a specific document by ID:
```shell
curl -X GET "localhost:9200/documents/_doc/{document_id}" -H "Content-Type: application/json"
```

To search for documents by user ID:
```shell
curl -X GET "localhost:9200/documents/_search" -H "Content-Type: application/json" -d'
{
  "query": {
    "term": {
      "userId": "your-user-id"
    }
  }
}'
```

```shell
curl -X GET "localhost:9200/documents/_search" -H "Content-Type: application/json" -d'
{
  "query": {
    "term": {
      "id": "678e97961e345b75cc5b7af2"
    }
  }
}'
```
